import numpy as np
import torch
import torch.nn as nn
import torch_npu
from parameter import Parameter as P # å¯¼å…¥è‡ªå®šä¹‰Parameterç±»

# è®¾ç½®éšæœºç§å­ä¿è¯å¯é‡å¤æ€§
torch.manual_seed(42)
if torch.npu.is_available():
    torch_npu.npu.manual_seed(42)

def compare_results(cpu_tensor, npu_tensor, name="Tensor", atol=1e-5):
    """å¢å¼ºç‰ˆç»“æœå¯¹æ¯”å‡½æ•°"""
    npu_tensor_cpu = npu_tensor.cpu().detach()
    cpu_tensor_detached = cpu_tensor.detach()
    
    max_diff = torch.max(torch.abs(npu_tensor_cpu - cpu_tensor_detached)).item()
    mean_diff = torch.mean(torch.abs(npu_tensor_cpu - cpu_tensor_detached)).item()
    
    print(f"\n[{name}]")
    print(f"Max difference: {max_diff:.2e}")
    print(f"Mean difference: {mean_diff:.2e}")
    
    if torch.allclose(npu_tensor_cpu, cpu_tensor_detached, atol=atol):
        print(f"âœ… Results match (atol={atol})")
    else:
        print(f"âŒ Results exceed tolerance (atol={atol})")
    return max_diff

def test_parameter_npu_cpu_consistency():
    # ç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„è®¾å¤‡
    device = torch.device("npu:0" if torch.npu.is_available() else "cpu")
    print(f"Testing device: {device.type.upper()}")

    class TestModel(nn.Module):
        def __init__(self):
            super().__init__()
            # ä½¿ç”¨è‡ªå®šä¹‰Parameterå¹¶æ˜¾å¼æŒ‡å®šè®¾å¤‡
            self.param = P(torch.randn(128, 256, device=device))  # å…³é”®ä¿®æ”¹ç‚¹1
            self.fc = nn.Linear(256, 128)
            self.relu = nn.ReLU()
            
        def forward(self, x):
            # æ·»åŠ è®¾å¤‡æ£€æŸ¥æ–­è¨€
            assert x.device == self.param.device, \
                f"Input device {x.device} vs param device {self.param.device}"
            x = torch.matmul(x, self.param)
            x = self.fc(x)
            x = self.relu(x)
            return x

    # åˆå§‹åŒ–æ¨¡å‹ï¼ˆæ˜¾å¼è®¾å¤‡åˆ†é…ï¼‰
    cpu_model = TestModel().to("cpu")
    npu_model = TestModel().to(device)
    
    # ç²¾ç¡®å‚æ•°åŒæ­¥ï¼ˆå¤„ç†è‡ªå®šä¹‰Parameterï¼‰
    with torch.no_grad():
        for (cpu_name, cpu_param), (npu_name, npu_param) in zip(
            cpu_model.named_parameters(),
            npu_model.named_parameters()
        ):
            # ç¡®ä¿å‚æ•°ç±»å‹æ­£ç¡®
            if isinstance(npu_param, P):
                npu_param.data = cpu_param.data.clone().to(device)  # å…³é”®ä¿®æ”¹ç‚¹2
            else:
                npu_param.copy_(cpu_param.data)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®ï¼ˆæ˜¾å¼è®¾å¤‡åˆ†é…ï¼‰
    x_cpu = torch.randn(32, 256, device="cpu", requires_grad=True)
    x_npu = x_cpu.detach().clone().to(device).requires_grad_(True)
    
    # å‰å‘è®¡ç®—å‰éªŒè¯è®¾å¤‡ä¸€è‡´æ€§
    print("\n[Device Check]")
    print(f"Input device: {x_npu.device}")
    for name, param in npu_model.named_parameters():
        print(f"Parameter '{name}' device: {param.device}")
    
    # æ‰§è¡Œå‰å‘ä¼ æ’­
    cpu_output = cpu_model(x_cpu)
    npu_output = npu_model(x_npu)
    
    # ç»“æœæ¯”è¾ƒ
    max_diff = compare_results(cpu_output, npu_output, "Forward Output", atol=1e-5)
    
    # åå‘ä¼ æ’­éªŒè¯
    target = torch.randn_like(cpu_output)
    cpu_loss = torch.mean((cpu_output - target)**2)
    npu_loss = torch.mean(npu_output - target.to(device))**2
    
    cpu_loss.backward()
    npu_loss.backward()
    
    # æ¢¯åº¦æ¯”è¾ƒ
    compare_results(x_cpu.grad, x_npu.grad, "Input Gradients")
    for i, (cpu_param, npu_param) in enumerate(zip(cpu_model.parameters(), npu_model.parameters())):
        compare_results(cpu_param.grad, npu_param.grad, f"Param Grad {i}")
    
    return max_diff < 1e-5

if __name__ == "__main__":
    if not torch.npu.is_available():
        print("Warning: NPU device not available, testing CPU vs CPU")
    
    try:
        test_passed = test_parameter_npu_cpu_consistency()
    except RuntimeError as e:
        print(f"\nâŒ Critical Error: {str(e)}")
        print("å»ºè®®æ£€æŸ¥ï¼š")
        print("1. NPUé©±åŠ¨å’ŒPyTorchç‰ˆæœ¬å…¼å®¹æ€§")
        print("2. è‡ªå®šä¹‰Parameterçš„è®¾å¤‡è¿ç§»é€»è¾‘")
        print("3. å¼ é‡è®¾å¤‡ä¸€è‡´æ€§ï¼ˆä½¿ç”¨tensor.deviceå±æ€§éªŒè¯ï¼‰")
        test_passed = False
    
    if test_passed:
        print("\nğŸ‰ Test Passed: NPU and CPU results match within tolerance!")
    else:
        print("\n  Test Warning: Numerical differences detected")
